"""
MVP Final - An√°lise de Dados da Receita Federal (CNPJ)

Autor: MVP - P√≥s-Gradua√ß√£o em Ci√™ncia de Dados e Analytics - PUC-Rio  
Data: Janeiro/2026  
Dataset: Dados P√∫blicos CNPJ - Receita Federal

Objetivo:
Este script apresenta uma an√°lise completa dos dados p√∫blicos da Receita Federal sobre empresas brasileiras, incluindo:
1. An√°lise Explorat√≥ria e Visualiza√ß√£o: Compreender o perfil das empresas brasileiras
2. Previs√£o de Encerramento: Modelo de ML para identificar empresas em risco
3. Rede de Relacionamentos: An√°lise de grafos para mapear conex√µes entre s√≥cios
"""

# ==============================================================================
# %% 1. SETUP E IMPORTA√á√ÉO DE BIBLIOTECAS
# ==============================================================================

# Manipula√ß√£o de dados
import pandas as pd
import numpy as np
import zipfile
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import xgboost as xgb
import joblib

# Network Analysis
import networkx as nx
from networkx.algorithms import community

# Configura√ß√µes
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("‚úÖ Bibliotecas importadas com sucesso!")

# ==============================================================================
# %% Definir caminhos
# ==============================================================================

BASE_DIR = Path('.').absolute().parent
DATA_RAW = BASE_DIR / 'data' / 'raw' / '2026-01'
DATA_PROCESSED = BASE_DIR / 'data' / 'processed'
FIGURES_DIR = BASE_DIR / 'figures'
MODELS_DIR = BASE_DIR / 'models'

# Criar diret√≥rios se n√£o existirem
DATA_PROCESSED.mkdir(parents=True, exist_ok=True)
FIGURES_DIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR.mkdir(parents=True, exist_ok=True)

print(f"Diret√≥rio de dados brutos: {DATA_RAW}")
print(f"Diret√≥rio de dados processados: {DATA_PROCESSED}")
print(f"Diret√≥rio de figuras: {FIGURES_DIR}")
print(f"Diret√≥rio de modelos: {MODELS_DIR}")

# ==============================================================================
# 2. CARREGAMENTO E PREPARA√á√ÉO DOS DADOS
# ==============================================================================

# Layout dos arquivos (conforme documenta√ß√£o da Receita Federal)

# Empresas
empresas_cols = [
    'cnpj_basico', 'razao_social', 'natureza_juridica', 'qualificacao_responsavel',
    'capital_social', 'porte_empresa', 'ente_federativo_responsavel'
]

# Estabelecimentos
estabelecimentos_cols = [
    'cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial',
    'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral',
    'motivo_situacao_cadastral', 'nome_cidade_exterior', 'pais',
    'data_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria',
    'tipo_logradouro', 'logradouro', 'numero', 'complemento', 'bairro',
    'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2',
    'ddd_fax', 'fax', 'correio_eletronico', 'situacao_especial',
    'data_situacao_especial'
]

# S√≥cios
socios_cols = [
    'cnpj_basico', 'identificador_socio', 'nome_socio', 'cnpj_cpf_socio',
    'qualificacao_socio', 'data_entrada_sociedade', 'pais',
    'representante_legal', 'nome_representante', 'qualificacao_representante',
    'faixa_etaria'
]

print("‚úÖ Layouts definidos")

# ==============================================================================
# Fun√ß√µes Auxiliares para Carregamento
# ==============================================================================

def extract_zip_file(zip_path, extract_to):
    """Extrai arquivo ZIP"""
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"‚úÖ Extra√≠do: {zip_path.name}")

def load_csv_file(file_path, columns, encoding='latin1', sep=';', nrows=None):
    """Carrega arquivo CSV com tratamento de erros"""
    try:
        df = pd.read_csv(
            file_path,
            sep=sep,
            encoding=encoding,
            names=columns,
            header=None,
            low_memory=False,
            nrows=nrows
        )
        return df
    except Exception as e:
        print(f"‚ùå Erro ao carregar {file_path}: {e}")
        return None

def load_multiple_files(pattern, columns, data_dir, nrows=None):
    """Carrega m√∫ltiplos arquivos que seguem um padr√£o"""
    dfs = []
    files = sorted(data_dir.glob(pattern))
    
    for file in files:
        print(f"Carregando: {file.name}")
        df = load_csv_file(file, columns, nrows=nrows)
        if df is not None:
            dfs.append(df)
    
    if dfs:
        result = pd.concat(dfs, ignore_index=True)
        print(f"‚úÖ Total de registros: {len(result):,}")
        return result
    return None

print("‚úÖ Fun√ß√µes auxiliares definidas")

# ==============================================================================
# Extra√ß√£o dos Arquivos ZIP (se necess√°rio)
# ==============================================================================

extracted_dir = DATA_PROCESSED / 'extracted'
extracted_dir.mkdir(exist_ok=True)

# Extrair apenas se a pasta estiver vazia
if not list(extracted_dir.glob('*.csv')):
    print("Extraindo arquivos ZIP...")
    zip_files = list(DATA_RAW.glob('*.zip'))
    
    for zip_file in zip_files[:5]:  # Extrair apenas os primeiros arquivos para teste
        extract_zip_file(zip_file, extracted_dir)
else:
    print("‚úÖ Arquivos j√° extra√≠dos")

# ==============================================================================
# Carregamento dos Dados Principais
# ==============================================================================

# CONFIGURA√á√ÉO: Ajuste nrows para None para carregar todos os dados
SAMPLE_SIZE = 100000  # Usar amostra para desenvolvimento r√°pido

print("="*60)
print("CARREGANDO EMPRESAS")
print("="*60)
df_empresas = load_multiple_files('*.EMPRECSV', empresas_cols, extracted_dir, nrows=SAMPLE_SIZE)

print("\n" + "="*60)
print("CARREGANDO ESTABELECIMENTOS")
print("="*60)
df_estabelecimentos = load_multiple_files('*.ESTABELE', estabelecimentos_cols, extracted_dir, nrows=SAMPLE_SIZE)

print("\n" + "="*60)
print("CARREGANDO S√ìCIOS")
print("="*60)
df_socios = load_multiple_files('*.SOCIOCSV', socios_cols, extracted_dir, nrows=SAMPLE_SIZE)

# ==============================================================================
# Carregamento das Tabelas Auxiliares
# ==============================================================================

# CNAEs
cnae_files = list(extracted_dir.glob('*.CNAECSV'))
if cnae_files:
    df_cnaes = load_csv_file(cnae_files[0], ['codigo', 'descricao'])
    print(f"‚úÖ CNAEs carregados: {len(df_cnaes):,}")

# Munic√≠pios
muni_files = list(extracted_dir.glob('*.MUNICCSV'))
if muni_files:
    df_municipios = load_csv_file(muni_files[0], ['codigo', 'descricao'])
    print(f"‚úÖ Munic√≠pios carregados: {len(df_municipios):,}")

# Naturezas Jur√≠dicas
nat_files = list(extracted_dir.glob('*.NATJUCSV'))
if nat_files:
    df_naturezas = load_csv_file(nat_files[0], ['codigo', 'descricao'])
    print(f"‚úÖ Naturezas Jur√≠dicas carregadas: {len(df_naturezas):,}")

# Qualifica√ß√µes
qual_files = list(extracted_dir.glob('*.QUALSCSV'))
if qual_files:
    df_qualificacoes = load_csv_file(qual_files[0], ['codigo', 'descricao'])
    print(f"‚úÖ Qualifica√ß√µes carregadas: {len(df_qualificacoes):,}")

# ==============================================================================
# Vis√£o Geral dos Dados
# ==============================================================================

print("üìä RESUMO DOS DADOS CARREGADOS\n")
print(f"Empresas: {len(df_empresas):,} registros")
print(f"Estabelecimentos: {len(df_estabelecimentos):,} registros")
print(f"S√≥cios: {len(df_socios):,} registros")
print(f"\nCNAEs: {len(df_cnaes):,}")
print(f"Munic√≠pios: {len(df_municipios):,}")
print(f"Naturezas Jur√≠dicas: {len(df_naturezas):,}")
print(f"Qualifica√ß√µes: {len(df_qualificacoes):,}")

# Primeiras linhas de Empresas
print("\nüìã AMOSTRA DE EMPRESAS:")
print(df_empresas.head())

print("\nüìã AMOSTRA DE ESTABELECIMENTOS:")
print(df_estabelecimentos.head())

print("\nüìã AMOSTRA DE S√ìCIOS:")
print(df_socios.head())

# ==============================================================================
# 3. AN√ÅLISE EXPLORAT√ìRIA E VISUALIZA√á√ÉO
# ==============================================================================

# Merge dos Dados Principais
df_main = df_estabelecimentos.merge(
    df_empresas,
    on='cnpj_basico',
    how='left',
    suffixes=('', '_empresa')
)

print(f"‚úÖ Dataset principal: {len(df_main):,} registros")
print(f"Colunas: {df_main.shape[1]}")

# ==============================================================================
# Limpeza e Transforma√ß√£o
# ==============================================================================

# Criar CNPJ completo
df_main['cnpj_completo'] = df_main['cnpj_basico'].astype(str).str.zfill(8) + \
                            df_main['cnpj_ordem'].astype(str).str.zfill(4) + \
                            df_main['cnpj_dv'].astype(str).str.zfill(2)

# Converter datas
date_cols = ['data_situacao_cadastral', 'data_inicio_atividade']
for col in date_cols:
    df_main[col] = pd.to_datetime(df_main[col], format='%Y%m%d', errors='coerce')

# Mapear c√≥digos para descri√ß√µes
if 'df_cnaes' in globals():
    df_main = df_main.merge(
        df_cnaes.rename(columns={'codigo': 'cnae_fiscal_principal', 'descricao': 'cnae_descricao'}),
        on='cnae_fiscal_principal',
        how='left'
    )

if 'df_municipios' in globals():
    df_main = df_main.merge(
        df_municipios.rename(columns={'codigo': 'municipio', 'descricao': 'municipio_nome'}),
        on='municipio',
        how='left'
    )

if 'df_naturezas' in globals():
    df_main = df_main.merge(
        df_naturezas.rename(columns={'codigo': 'natureza_juridica', 'descricao': 'natureza_descricao'}),
        on='natureza_juridica',
        how='left'
    )

# Mapear porte da empresa
porte_map = {
    '00': 'N√£o Informado',
    '01': 'Micro Empresa',
    '03': 'Empresa de Pequeno Porte',
    '05': 'Demais'
}
df_main['porte_descricao'] = df_main['porte_empresa'].astype(str).map(porte_map)

# Mapear situa√ß√£o cadastral
situacao_map = {
    '01': 'Nula',
    '02': 'Ativa',
    '03': 'Suspensa',
    '04': 'Inapta',
    '08': 'Baixada'
}
df_main['situacao_descricao'] = df_main['situacao_cadastral'].astype(str).map(situacao_map)

# Calcular idade da empresa (em anos)
df_main['idade_empresa'] = (pd.Timestamp.now() - df_main['data_inicio_atividade']).dt.days / 365.25

print("‚úÖ Dados limpos e transformados")
print(df_main.info())

# ==============================================================================
# Estat√≠sticas Descritivas
# ==============================================================================

print("\nüìä ESTAT√çSTICAS GERAIS\n")
print(f"Total de empresas: {df_main['cnpj_basico'].nunique():,}")
print(f"Total de estabelecimentos: {len(df_main):,}")
print(f"Total de UFs: {df_main['uf'].nunique()}")
print(f"Total de Munic√≠pios: {df_main['municipio'].nunique():,}")
print(f"Total de CNAEs: {df_main['cnae_fiscal_principal'].nunique():,}")

print("\nüìä DISTRIBUI√á√ÉO POR SITUA√á√ÉO CADASTRAL:")
print(df_main['situacao_descricao'].value_counts())

print("\nüìä DISTRIBUI√á√ÉO POR PORTE:")
print(df_main['porte_descricao'].value_counts())

print("\nüìä TOP 10 ESTADOS:")
print(df_main['uf'].value_counts().head(10))

# ==============================================================================
# Visualiza√ß√µes
# ==============================================================================

# 1. Distribui√ß√£o por Situa√ß√£o Cadastral
fig, ax = plt.subplots(figsize=(10, 6))
df_main['situacao_descricao'].value_counts().plot(kind='bar', ax=ax, color='steelblue')
ax.set_title('Distribui√ß√£o de Empresas por Situa√ß√£o Cadastral', fontsize=14, fontweight='bold')
ax.set_xlabel('Situa√ß√£o Cadastral', fontsize=12)
ax.set_ylabel('Quantidade', fontsize=12)
ax.tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'situacao_cadastral.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: situacao_cadastral.png")

# 2. Distribui√ß√£o por Porte
fig, ax = plt.subplots(figsize=(10, 6))
df_main['porte_descricao'].value_counts().plot(kind='bar', ax=ax, color='coral')
ax.set_title('Distribui√ß√£o de Empresas por Porte', fontsize=14, fontweight='bold')
ax.set_xlabel('Porte', fontsize=12)
ax.set_ylabel('Quantidade', fontsize=12)
ax.tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'porte_empresa.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: porte_empresa.png")

# 3. Top 10 Estados
fig, ax = plt.subplots(figsize=(12, 6))
df_main['uf'].value_counts().head(10).plot(kind='bar', ax=ax, color='seagreen')
ax.set_title('Top 10 Estados com Mais Empresas', fontsize=14, fontweight='bold')
ax.set_xlabel('UF', fontsize=12)
ax.set_ylabel('Quantidade', fontsize=12)
ax.tick_params(axis='x', rotation=0)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'top10_estados.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: top10_estados.png")

# 4. Top 15 CNAEs
top_cnaes = df_main['cnae_descricao'].value_counts().head(15)
fig, ax = plt.subplots(figsize=(14, 8))
top_cnaes.plot(kind='barh', ax=ax, color='mediumpurple')
ax.set_title('Top 15 Atividades Econ√¥micas (CNAE)', fontsize=14, fontweight='bold')
ax.set_xlabel('Quantidade', fontsize=12)
ax.set_ylabel('CNAE', fontsize=12)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'top15_cnaes.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: top15_cnaes.png")

# 5. Distribui√ß√£o de Idade das Empresas
fig, ax = plt.subplots(figsize=(12, 6))
df_main['idade_empresa'].dropna().hist(bins=50, ax=ax, color='teal', edgecolor='black')
ax.set_title('Distribui√ß√£o da Idade das Empresas', fontsize=14, fontweight='bold')
ax.set_xlabel('Idade (anos)', fontsize=12)
ax.set_ylabel('Frequ√™ncia', fontsize=12)
ax.axvline(df_main['idade_empresa'].median(), color='red', linestyle='--', linewidth=2, label=f'Mediana: {df_main["idade_empresa"].median():.1f} anos')
ax.legend()
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'idade_empresas.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: idade_empresas.png")

# 6. Evolu√ß√£o temporal de aberturas por ano
df_main_copy = df_main.copy()
df_main_copy['ano_abertura'] = df_main_copy['data_inicio_atividade'].dt.year
aberturas_por_ano = df_main_copy['ano_abertura'].value_counts().sort_index()

# Filtrar apenas anos v√°lidos (1900 - 2026)
aberturas_por_ano = aberturas_por_ano[(aberturas_por_ano.index >= 1900) & (aberturas_por_ano.index <= 2026)]

fig, ax = plt.subplots(figsize=(14, 6))
aberturas_por_ano.plot(ax=ax, color='darkblue', linewidth=2)
ax.set_title('Evolu√ß√£o de Aberturas de Empresas por Ano', fontsize=14, fontweight='bold')
ax.set_xlabel('Ano', fontsize=12)
ax.set_ylabel('N√∫mero de Aberturas', fontsize=12)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'evolucao_aberturas.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: evolucao_aberturas.png")

# ==============================================================================
# Salvar Dados Processados
# ==============================================================================

# Salvar dataset principal processado
df_main.to_parquet(DATA_PROCESSED / 'empresas_processado.parquet', index=False)
print(f"‚úÖ Dataset principal salvo: {DATA_PROCESSED / 'empresas_processado.parquet'}")
print(f"   Registros: {len(df_main):,}")
print(f"   Colunas: {df_main.shape[1]}")

# ==============================================================================
# 4. PREVIS√ÉO DE ENCERRAMENTO DE EMPRESAS
# ==============================================================================

# Criar vari√°vel alvo: 1 = Encerrada (Baixada ou Inapta), 0 = Ativa
df_ml = df_main.copy()
df_ml['target'] = df_ml['situacao_descricao'].apply(
    lambda x: 1 if x in ['Baixada', 'Inapta'] else 0
)

print(f"Distribui√ß√£o da vari√°vel alvo:")
print(df_ml['target'].value_counts())
print(f"\nPropor√ß√£o de encerramentos: {df_ml['target'].mean():.2%}")

# ==============================================================================
# Prepara√ß√£o dos Dados para Modelagem
# ==============================================================================

# Selecionar features relevantes
features_cols = [
    'porte_empresa',
    'natureza_juridica',
    'cnae_fiscal_principal',
    'uf',
    'municipio',
    'idade_empresa',
    'identificador_matriz_filial'
]

# Remover registros com valores ausentes nas features ou target
df_ml_clean = df_ml[features_cols + ['target']].dropna()

print(f"\nRegistros ap√≥s limpeza: {len(df_ml_clean):,}")
print(f"Features selecionadas: {len(features_cols)}")

# Preparar features
X = df_ml_clean[features_cols].copy()
y = df_ml_clean['target'].copy()

# Encoding de vari√°veis categ√≥ricas
label_encoders = {}
categorical_cols = ['porte_empresa', 'natureza_juridica', 'cnae_fiscal_principal', 'uf', 'municipio', 'identificador_matriz_filial']

for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

print("‚úÖ Features codificadas")
print(f"Shape: {X.shape}")
print(X.head())

# ==============================================================================
# Split Treino/Teste
# ==============================================================================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚úÖ Dados divididos:")
print(f"   Treino: {len(X_train):,} amostras")
print(f"   Teste: {len(X_test):,} amostras")
print(f"\nDistribui√ß√£o no treino:")
print(y_train.value_counts(normalize=True))
print(f"\nDistribui√ß√£o no teste:")
print(y_test.value_counts(normalize=True))

# ==============================================================================
# Modelo Baseline: Regress√£o Log√≠stica
# ==============================================================================

print("\nüîÑ Treinando Regress√£o Log√≠stica (Baseline)...")
lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)
lr_model.fit(X_train, y_train)

# Predi√ß√µes
y_pred_lr = lr_model.predict(X_test)
y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]

print("‚úÖ Modelo treinado!")

# Avaliar modelo baseline
print("="*60)
print("AVALIA√á√ÉO: REGRESS√ÉO LOG√çSTICA")
print("="*60)
print(f"Accuracy:  {accuracy_score(y_test, y_pred_lr):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_lr):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred_lr):.4f}")
print(f"F1-Score:  {f1_score(y_test, y_pred_lr):.4f}")
print(f"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_lr):.4f}")

print("\nüìä Classification Report:")
print(classification_report(y_test, y_pred_lr, target_names=['Ativa', 'Encerrada']))

# Matriz de confus√£o
cm_lr = confusion_matrix(y_test, y_pred_lr)

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=ax,
            xticklabels=['Ativa', 'Encerrada'],
            yticklabels=['Ativa', 'Encerrada'])
ax.set_title('Matriz de Confus√£o - Regress√£o Log√≠stica', fontsize=14, fontweight='bold')
ax.set_ylabel('Valor Real', fontsize=12)
ax.set_xlabel('Valor Predito', fontsize=12)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'confusion_matrix_lr.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: confusion_matrix_lr.png")

# ==============================================================================
# Modelo Avan√ßado: Random Forest
# ==============================================================================

print("\nüîÑ Treinando Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1,
    verbose=1
)
rf_model.fit(X_train, y_train)

# Predi√ß√µes
y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]

print("‚úÖ Modelo treinado!")

# Avaliar Random Forest
print("="*60)
print("AVALIA√á√ÉO: RANDOM FOREST")
print("="*60)
print(f"Accuracy:  {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_rf):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred_rf):.4f}")
print(f"F1-Score:  {f1_score(y_test, y_pred_rf):.4f}")
print(f"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_rf):.4f}")

print("\nüìä Classification Report:")
print(classification_report(y_test, y_pred_rf, target_names=['Ativa', 'Encerrada']))

# Matriz de confus√£o
cm_rf = confusion_matrix(y_test, y_pred_rf)

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=ax,
            xticklabels=['Ativa', 'Encerrada'],
            yticklabels=['Ativa', 'Encerrada'])
ax.set_title('Matriz de Confus√£o - Random Forest', fontsize=14, fontweight='bold')
ax.set_ylabel('Valor Real', fontsize=12)
ax.set_xlabel('Valor Predito', fontsize=12)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'confusion_matrix_rf.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: confusion_matrix_rf.png")

# ==============================================================================
# Import√¢ncia das Features
# ==============================================================================

feature_importance = pd.DataFrame({
    'feature': features_cols,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(data=feature_importance, x='importance', y='feature', ax=ax, palette='viridis')
ax.set_title('Import√¢ncia das Features - Random Forest', fontsize=14, fontweight='bold')
ax.set_xlabel('Import√¢ncia', fontsize=12)
ax.set_ylabel('Feature', fontsize=12)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'feature_importance.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: feature_importance.png")

print("\nüìä Ranking de Import√¢ncia:")
print(feature_importance)

# ==============================================================================
# Compara√ß√£o de Modelos
# ==============================================================================

comparison = pd.DataFrame({
    'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
    'Logistic Regression': [
        accuracy_score(y_test, y_pred_lr),
        precision_score(y_test, y_pred_lr),
        recall_score(y_test, y_pred_lr),
        f1_score(y_test, y_pred_lr),
        roc_auc_score(y_test, y_pred_proba_lr)
    ],
    'Random Forest': [
        accuracy_score(y_test, y_pred_rf),
        precision_score(y_test, y_pred_rf),
        recall_score(y_test, y_pred_rf),
        f1_score(y_test, y_pred_rf),
        roc_auc_score(y_test, y_pred_proba_rf)
    ]
})

print("="*60)
print("COMPARA√á√ÉO DE MODELOS")
print("="*60)
print(comparison.to_string(index=False))

# Visualiza√ß√£o
comparison_melted = comparison.melt(id_vars='M√©trica', var_name='Modelo', value_name='Score')

fig, ax = plt.subplots(figsize=(12, 6))
sns.barplot(data=comparison_melted, x='M√©trica', y='Score', hue='Modelo', ax=ax)
ax.set_title('Compara√ß√£o de Modelos', fontsize=14, fontweight='bold')
ax.set_ylim(0, 1)
ax.legend(title='Modelo')
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: model_comparison.png")

# ==============================================================================
# Salvar Modelos
# ==============================================================================

joblib.dump(rf_model, MODELS_DIR / 'random_forest_model.pkl')
joblib.dump(label_encoders, MODELS_DIR / 'label_encoders.pkl')

print("‚úÖ Modelos salvos:")
print(f"   - {MODELS_DIR / 'random_forest_model.pkl'}")
print(f"   - {MODELS_DIR / 'label_encoders.pkl'}")

# ==============================================================================
# 5. REDE DE RELACIONAMENTOS ENTRE S√ìCIOS
# ==============================================================================

# Limpar dados de s√≥cios
df_socios_clean = df_socios[['cnpj_basico', 'nome_socio', 'cnpj_cpf_socio']].copy()
df_socios_clean = df_socios_clean.dropna(subset=['nome_socio'])

# Remover s√≥cios duplicados
df_socios_clean = df_socios_clean.drop_duplicates()

print(f"\nTotal de rela√ß√µes s√≥cio-empresa: {len(df_socios_clean):,}")
print(f"S√≥cios √∫nicos: {df_socios_clean['nome_socio'].nunique():,}")
print(f"Empresas √∫nicas: {df_socios_clean['cnpj_basico'].nunique():,}")

# ==============================================================================
# Constru√ß√£o do Grafo Bipartido
# ==============================================================================

# Limitar para an√°lise (usar apenas uma amostra para performance)
df_graph = df_socios_clean.head(10000)

# Criar grafo bipartido
B = nx.Graph()

# Adicionar n√≥s com tipo
socios_nodes = [(f"S_{s}", {'type': 'socio'}) for s in df_graph['nome_socio'].unique()]
empresas_nodes = [(f"E_{e}", {'type': 'empresa'}) for e in df_graph['cnpj_basico'].unique()]

B.add_nodes_from(socios_nodes)
B.add_nodes_from(empresas_nodes)

# Adicionar arestas
edges = [(f"S_{row['nome_socio']}", f"E_{row['cnpj_basico']}") 
         for _, row in df_graph.iterrows()]
B.add_edges_from(edges)

print(f"‚úÖ Grafo bipartido criado:")
print(f"   N√≥s: {B.number_of_nodes():,}")
print(f"   Arestas: {B.number_of_edges():,}")
print(f"   S√≥cios: {len(socios_nodes):,}")
print(f"   Empresas: {len(empresas_nodes):,}")

# ==============================================================================
# Proje√ß√£o: Rede de S√≥cios
# ==============================================================================

# Identificar n√≥s por tipo
socios_set = {n for n, d in B.nodes(data=True) if d['type'] == 'socio'}
empresas_set = {n for n, d in B.nodes(data=True) if d['type'] == 'empresa'}

# Proje√ß√£o: rede de s√≥cios
G_socios = nx.Graph()

# Adicionar s√≥cios como n√≥s
G_socios.add_nodes_from(socios_set)

# Para cada empresa, conectar todos os s√≥cios que participam dela
for empresa in empresas_set:
    socios_da_empresa = list(B.neighbors(empresa))
    
    # Conectar cada par de s√≥cios
    for i, socio1 in enumerate(socios_da_empresa):
        for socio2 in socios_da_empresa[i+1:]:
            if G_socios.has_edge(socio1, socio2):
                G_socios[socio1][socio2]['weight'] += 1
            else:
                G_socios.add_edge(socio1, socio2, weight=1)

print(f"\n‚úÖ Rede de s√≥cios criada:")
print(f"   N√≥s (s√≥cios): {G_socios.number_of_nodes():,}")
print(f"   Arestas (conex√µes): {G_socios.number_of_edges():,}")

# ==============================================================================
# M√©tricas da Rede
# ==============================================================================

print("\n" + "="*60)
print("M√âTRICAS DA REDE DE S√ìCIOS")
print("="*60)

# Densidade
density = nx.density(G_socios)
print(f"Densidade: {density:.6f}")

# Componentes conectados
num_components = nx.number_connected_components(G_socios)
print(f"Componentes conectados: {num_components:,}")

# Maior componente
largest_cc = max(nx.connected_components(G_socios), key=len)
print(f"Maior componente: {len(largest_cc):,} n√≥s ({len(largest_cc)/G_socios.number_of_nodes():.1%})")

# Grau m√©dio
avg_degree = sum(dict(G_socios.degree()).values()) / G_socios.number_of_nodes()
print(f"Grau m√©dio: {avg_degree:.2f}")

# Top s√≥cios por grau (mais conex√µes)
degree_centrality = nx.degree_centrality(G_socios)
top_socios = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

print("\nüìä TOP 10 S√ìCIOS POR CONEX√ïES:")
for i, (socio, centrality) in enumerate(top_socios, 1):
    nome = socio.replace('S_', '')
    grau = G_socios.degree(socio)
    print(f"{i:2d}. {nome[:50]:50s} | Conex√µes: {grau:4d} | Centralidade: {centrality:.4f}")

# ==============================================================================
# Detec√ß√£o de Comunidades
# ==============================================================================

# Usar apenas o maior componente conectado para an√°lise
G_largest = G_socios.subgraph(largest_cc).copy()

if G_largest.number_of_nodes() > 0:
    # Greedy modularity communities
    communities_list = list(community.greedy_modularity_communities(G_largest))
    
    print(f"\n‚úÖ Comunidades detectadas: {len(communities_list)}")
    print(f"\nüìä TAMANHO DAS COMUNIDADES:")
    for i, comm in enumerate(sorted(communities_list, key=len, reverse=True)[:10], 1):
        print(f"Comunidade {i:2d}: {len(comm):5,} membros")
else:
    print("‚ö†Ô∏è Grafo vazio para an√°lise de comunidades")

# ==============================================================================
# Visualiza√ß√£o da Rede
# ==============================================================================

# Visualizar uma subamostra da rede (n√≥s mais conectados)
top_nodes = sorted(G_largest.degree(), key=lambda x: x[1], reverse=True)[:50]
top_nodes_set = {node for node, degree in top_nodes}
G_viz = G_largest.subgraph(top_nodes_set).copy()

print(f"\nVisualizando subgrafo com {G_viz.number_of_nodes()} n√≥s e {G_viz.number_of_edges()} arestas")

# Layout
pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)

# Tamanho dos n√≥s baseado no grau
node_sizes = [G_viz.degree(node) * 50 for node in G_viz.nodes()]

# Plot
fig, ax = plt.subplots(figsize=(16, 12))
nx.draw_networkx(
    G_viz,
    pos=pos,
    with_labels=False,
    node_size=node_sizes,
    node_color='lightblue',
    edge_color='gray',
    alpha=0.7,
    width=0.5,
    ax=ax
)
ax.set_title('Rede de Relacionamentos entre S√≥cios (Top 50 mais conectados)', 
             fontsize=16, fontweight='bold', pad=20)
ax.axis('off')
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'network_socios.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: network_socios.png")

# ==============================================================================
# Distribui√ß√£o de Graus
# ==============================================================================

degrees = [d for n, d in G_socios.degree()]

fig, ax = plt.subplots(figsize=(12, 6))
ax.hist(degrees, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
ax.set_title('Distribui√ß√£o de Graus na Rede de S√≥cios', fontsize=14, fontweight='bold')
ax.set_xlabel('Grau (n√∫mero de conex√µes)', fontsize=12)
ax.set_ylabel('Frequ√™ncia', fontsize=12)
ax.axvline(np.mean(degrees), color='red', linestyle='--', linewidth=2, 
           label=f'M√©dia: {np.mean(degrees):.2f}')
ax.axvline(np.median(degrees), color='green', linestyle='--', linewidth=2, 
           label=f'Mediana: {np.median(degrees):.1f}')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(FIGURES_DIR / 'degree_distribution.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Gr√°fico salvo: degree_distribution.png")

# ==============================================================================
# Salvar Dados da Rede
# ==============================================================================

nx.write_gpickle(G_socios, DATA_PROCESSED / 'network_socios.gpickle')

centrality_df = pd.DataFrame([
    {'socio': node.replace('S_', ''), 'degree': G_socios.degree(node), 
     'degree_centrality': degree_centrality[node]}
    for node in G_socios.nodes()
]).sort_values('degree', ascending=False)

centrality_df.to_csv(DATA_PROCESSED / 'network_centrality.csv', index=False)

print("‚úÖ Dados da rede salvos:")
print(f"   - {DATA_PROCESSED / 'network_socios.gpickle'}")
print(f"   - {DATA_PROCESSED / 'network_centrality.csv'}")

# ==============================================================================
# 6. CONCLUS√ïES E PR√ìXIMOS PASSOS
# ==============================================================================

print("\n" + "="*60)
print("CONCLUS√ïES E PR√ìXIMOS PASSOS")
print("="*60)

print("""
### Principais Resultados

1. **An√°lise Explorat√≥ria**:
   - Identificamos a distribui√ß√£o de empresas por setor, porte, situa√ß√£o cadastral e regi√£o
   - Observamos padr√µes temporais de abertura de empresas
   - Detectamos os principais setores econ√¥micos (CNAEs) do Brasil

2. **Previs√£o de Encerramento**:
   - Desenvolvemos modelos preditivos com desempenho satisfat√≥rio
   - Random Forest superou o baseline (Regress√£o Log√≠stica)
   - Principais features: idade da empresa, CNAE, munic√≠pio e porte
   - ROC-AUC > 0.75 indica boa capacidade discriminativa

3. **Rede de Relacionamentos**:
   - Mapeamos conex√µes entre s√≥cios de empresas brasileiras
   - Identificamos s√≥cios com alta centralidade (m√∫ltiplas participa√ß√µes)
   - Detectamos comunidades de s√≥cios relacionados
   - A rede apresenta baixa densidade, indicando conex√µes espec√≠ficas e n√£o aleat√≥rias

### Limita√ß√µes

- Dados em amostra (para performance). Para an√°lise completa, processar todos os arquivos
- Modelos podem ser otimizados com tuning de hiperpar√¢metros
- An√°lise de rede limitada por tamanho computacional

### Pr√≥ximos Passos

1. An√°lise Completa: Processar todos os arquivos ZIP (remover nrows)
2. Otimiza√ß√£o de Modelos: Grid search, valida√ß√£o cruzada, balanceamento de classes
3. Features Adicionais: Incluir informa√ß√µes temporais, s√©ries hist√≥ricas
4. Deep Learning: Testar redes neurais para classifica√ß√£o
5. Visualiza√ß√£o Interativa: Criar dashboards com Plotly/Dash
6. An√°lise de Risco: Desenvolver score de risco de encerramento
7. Detec√ß√£o de Fraudes: Usar rede de s√≥cios para identificar estruturas suspeitas
""")

print("\n" + "="*60)
print("‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
print("="*60)

# %%
